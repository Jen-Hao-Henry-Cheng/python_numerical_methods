{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2ed1dc-935a-4829-b815-1ec7e6d92d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sym\n",
    "import matplotlib.pyplot as plt\n",
    "from newton_raphson import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9b0668-9d6c-496e-91e2-c098960ed391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_by_difference(symFunc, valueXk, delta):\n",
    "    fk = symFunc.subs({x1: valueXk[0], x2: valueXk[1], x3: valueXk[2]})\n",
    "    fk_x1 = f.subs({x1: valueXk[0] + delta, x2: valueXk[1], x3: valueXk[2]})\n",
    "    fk_x2 = f.subs({x1: valueXk[0], x2: valueXk[1] + delta, x3: valueXk[2]})\n",
    "    fk_x3 = f.subs({x1: valueXk[0], x2: valueXk[1], x3: valueXk[2] + delta})\n",
    "    grad = np.array([(fk_x1 - fk)/delta, (fk_x2 - fk)/delta, (fk_x3 - fk)/delta])\n",
    "    return grad\n",
    "\n",
    "def find_optimal_alpha(symFunc, symVar):\n",
    "    f_1st_derv = sym.diff(symFunc, symVar)\n",
    "    f_2nd_derv = sym.diff(f_1st_derv, symVar)\n",
    "\n",
    "    f_1st_derv = sym.lambdify(alpha, f_1st_derv, 'numpy')\n",
    "    f_2nd_derv = sym.lambdify(alpha, f_2nd_derv, 'numpy')\n",
    "\n",
    "    optValue, Iter = newton_raphson(f_1st_derv, f_2nd_derv, 0)\n",
    "    return optValue, Iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b1e6bb-9a22-4360-ab3e-5d7676c2b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 3 x_{1}^{2} - 5 x_{1} + x_{2}^{2} + 2 x_{2} + 2 x_{3}^{2}$"
      ],
      "text/plain": [
       "3*x1**2 - 5*x1 + x2**2 + 2*x2 + 2*x3**2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2, x3, alpha = sym.symbols('x1 x2 x3 alpha')\n",
    "A = 3\n",
    "B = 2\n",
    "C = 2\n",
    "f = A*x1**2 + x2**2 + B*x3**2 - 5*x1 + 2*x2\n",
    "f # display f(x1, x2, x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9783d23-2c2e-4cc2-a899-95b54e093710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intial guess x0: [[2. 2. 3.]]\n"
     ]
    }
   ],
   "source": [
    "maxIter = 100;\n",
    "delta = 1.0e-05\n",
    "epsilon = 1.0e-04\n",
    "\n",
    "x = np.empty(shape=[0, 3])\n",
    "x = np.append(x, [[C, B, A]], axis=0) # initial value\n",
    "print(f'intial guess x0: {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188d8b11-a932-49bd-8605-9bade59fe878",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 0, grad: [7.00002999991511 6.00000999995132 12.0000199999026], x: [0.298298857709230, 0.541402841016822, 0.0828056820336438], alpha: 0.24309912133396663, error: 6.07749261924030\n",
      "k: 1, grad: [-3.21017685374070 3.08281568200952 0.331242728135450], x: [1.08501756981239, -0.214103442126377, 0.00162795745794068], alpha: 0.24507020888473152, error: 1.62340271982207\n",
      "k: 2, grad: [1.51013541880296 1.57180311575189 0.00653182983256784], x: [0.699783430101515, -0.615068953928973, -3.83062525560198e-5], alpha: 0.2550990692054897, error: 0.787865915223970\n",
      "k: 3, grad: [-0.801269419348216 0.769872092165613 -0.000133224986598179], x: [0.896180254434137, -0.803770070423136, -5.65186233740630e-6], alpha: 0.2451071007956083, error: 0.385130595217003\n",
      "k: 4, grad: [0.377111526583818 0.392469859145095 -2.60746979563464e-6], x: [0.799988309145396, -0.903879551457453, -4.98676043984342e-6], alpha: 0.25507559039662875, error: 0.196302091424955\n",
      "k: 5, grad: [-0.200040145115210 0.192250897068646 5.29354338141275e-8], x: [0.849027663942955, -0.951009391050743, -4.99973743262993e-6], alpha: 0.24514756660127443, error: 0.0961692073678430\n",
      "k: 6, grad: [0.0941959837241768 0.0979912179399633 1.06581410364015e-9], x: [0.825009401064784, -0.975995369636899, -5.00000919584433e-6], alpha: 0.25498181481389487, error: 0.0490042417360896\n",
      "k: 7, grad: [-0.0499135936138373 0.0480192607010821 0], x: [0.837253690504509, -0.987774960809085, -5.00000919584433e-6], alpha: 0.24530971531431095, error: 0.0240238806119107\n",
      "k: 8, grad: [0.0235521429914343 0.0244600784071736 0], x: [0.831257155100416, -0.994002662730259, -5.00000919584433e-6], alpha: 0.2546067848804305, error: 0.0122242373252670\n",
      "k: 9, grad: [-0.0124270694179529 0.0120046745877289 0], x: [0.834313724285771, -0.996955339438740, -5.00000919584433e-6], alpha: 0.2459605786815144, error: 0.00600924589383611\n",
      "k: 10, grad: [0.00591234567970389 0.00609932109263411 0], x: [0.832817231546227, -0.998499158123531, -5.00000919584433e-6], alpha: 0.2531132008537466, error: 0.00304031142433503\n",
      "k: 11, grad: [-0.00306661069870984 0.00301168379124306 0], x: [0.833579563888210, -0.999247836121314, -5.00000919584433e-6], alpha: 0.24859117014856136, error: 0.00151101033976497\n",
      "k: 12, grad: [0.00150738328486000 0.00151432777428795 0], x: [0.833206844235061, -0.999622272887633, -5.00000919584433e-6], alpha: 0.2472626948250423, error: 0.000747156419467543\n",
      "k: 13, grad: [-0.000728934601568198 0.000765454233331297 0], x: [0.833396016400142, -0.999820922580319, -5.00000919584433e-6], alpha: 0.25951870671867266, error: 0.000387821857767978\n",
      "k: 14, grad: [0.000406098354943651 0.000368154884355931 0], x: [0.833304334726110, -0.999904038051461, -5.00000919584433e-6], alpha: 0.22576223940898674, error: 0.000174797145173988\n",
      "k: 15, grad: [-0.000143991618628547 0.000201923899822987 0], x: [0.833348946585790, -0.999966598641936, -5.00000919584433e-6], alpha: 0.3098226140145915, error: 0.000107172450154902\n",
      "k: 16, grad: [0.000123679644303820 7.68027419439932e-5 0], x: [0.833328985272874, -0.999978994243276, -5.00000919584433e-6], alpha: 0.16139529692001953, error: 0.0000323569142557378\n",
      "[0.833328985272874, -0.999978994243276, -5.00000919584433e-6]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "for k in range(maxIter):\n",
    "    # get gradient by finite difference\n",
    "    grad = get_grad_by_difference(f, x[k], delta)\n",
    "\n",
    "    # find optimal alpha\n",
    "    x_alpha = x[k] - alpha*grad\n",
    "    f_alpha = f.subs({x1: x_alpha[0], x2: x_alpha[1], x3: x_alpha[2]})\n",
    "    opt_alpha, Iter = find_optimal_alpha(f_alpha, alpha)\n",
    "    \n",
    "    # compute new x\n",
    "    x_new = [x_alpha[0].subs(alpha, opt_alpha), x_alpha[1].subs(alpha, opt_alpha), x_alpha[2].subs(alpha, opt_alpha)]\n",
    "\n",
    "    # save new x into array x\n",
    "    x = np.append(x, [x_new], axis=0)\n",
    "    \n",
    "    # error check\n",
    "    error = np.linalg.norm(x_new - x[k], 1)\n",
    "    print(f'k: {k}, grad: {grad}, x: {x_new}, alpha: {opt_alpha}, error: {error}')\n",
    "    if error < epsilon:\n",
    "        print(x_new)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
